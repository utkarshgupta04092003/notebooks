{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOLlGya7vIPKGv4HfuefxHa",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/utkarshgupta04092003/notebooks/blob/main/create-neural-network/Pretraining_LLM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Install dependencies and fix seed\n"
      ],
      "metadata": {
        "id": "_MygO-9tpm3k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install \\\n",
        "  accelerate==0.26.1 \\\n",
        "  datasets==2.16.1 \\\n",
        "  fasttext==0.9.2 \\\n",
        "  jupyter==1.0.0 \\\n",
        "  pandas==2.2.0 \\\n",
        "  pyarrow==15.0.0 \\\n",
        "  sentencepiece==0.1.99 \\\n",
        "  torch==2.1.2 \\\n",
        "  torchaudio==2.1.2 \\\n",
        "  torchvision==0.16.2 \\\n",
        "  tqdm==4.66.1 \\\n",
        "  transformers==4.37.2\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-dXAF2G9q4U_",
        "outputId": "b851b85d-8882-4ab9-90a2-ba95686aa06b"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting accelerate==0.26.1\n",
            "  Downloading accelerate-0.26.1-py3-none-any.whl.metadata (18 kB)\n",
            "Collecting datasets==2.16.1\n",
            "  Downloading datasets-2.16.1-py3-none-any.whl.metadata (20 kB)\n",
            "Collecting fasttext==0.9.2\n",
            "  Downloading fasttext-0.9.2.tar.gz (68 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m68.8/68.8 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting jupyter==1.0.0\n",
            "  Downloading jupyter-1.0.0-py2.py3-none-any.whl.metadata (995 bytes)\n",
            "Collecting pandas==2.2.0\n",
            "  Downloading pandas-2.2.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (19 kB)\n",
            "Collecting pyarrow==15.0.0\n",
            "  Downloading pyarrow-15.0.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (3.0 kB)\n",
            "Collecting sentencepiece==0.1.99\n",
            "  Downloading sentencepiece-0.1.99.tar.gz (2.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m45.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement torch==2.1.2 (from versions: 2.2.0, 2.2.1, 2.2.2, 2.3.0, 2.3.1, 2.4.0, 2.4.1, 2.5.0, 2.5.1, 2.6.0, 2.7.0, 2.7.1, 2.8.0)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for torch==2.1.2\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "_exbJ_tjl4OM"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch"
      ],
      "metadata": {
        "id": "IO3l8XhKom9e"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fix_torch_seed(seed=42):\n",
        "  torch.manual_seed(seed)\n",
        "  torch.cuda.manual_seed(seed)\n",
        "  torch.backends.cudnn.deterministic=True\n",
        "  torch.backends.cudnn.benchmark=False\n",
        "\n",
        "fix_torch_seed()"
      ],
      "metadata": {
        "id": "xmtbDTa2ooup"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Load a general pretrained model"
      ],
      "metadata": {
        "id": "zuUyhZ1spn6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_path_or_name = \"upstage/TinySolar-248m-4k\""
      ],
      "metadata": {
        "id": "Cn_Qf_h2pB7u"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM\n",
        "tiny_general_model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_path_or_name,\n",
        "    device_map=\"cpu\", # change to auto if you have access to a GPU\n",
        "    torch_dtype=torch.bfloat16\n",
        ")"
      ],
      "metadata": {
        "id": "LgQmWUqalN8j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "tiny_general_tokenizer = AutoTokenizer.from_pretrained(model_path_or_name)"
      ],
      "metadata": {
        "id": "lbyoDxFgrDZ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Generate Text Sample"
      ],
      "metadata": {
        "id": "CfmMRd136k3l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"I am an engineer. I love\""
      ],
      "metadata": {
        "id": "4SANBPtW6oo5"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = tiny_general_tokenizer(prompt, return_tensors=\"pt\")"
      ],
      "metadata": {
        "id": "hbNtxcVI6q4g"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TextStreamer\n",
        "streamer = TextStreamer(\n",
        "    tiny_general_tokenizer,\n",
        "    skip_prompt=True, # If you set to false, the model will first return the prompt and then the generated text\n",
        "    skip_special_tokens=True\n",
        ")"
      ],
      "metadata": {
        "id": "nD0h6Rhv69-s"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "outputs = tiny_general_model.generate(\n",
        "    **inputs,\n",
        "    streamer=streamer,\n",
        "    use_cache=True,\n",
        "    max_new_tokens=128,\n",
        "    do_sample=False,\n",
        "    temperature=0.0,\n",
        "    repetition_penalty=1.1\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8pAZuJP56_5I",
        "outputId": "98a5a9ef-49a9-4217-d09a-037a2b03a1d0"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "to travel and have a great time, but I'm not sure if I can do it all again.\n",
            "I've been working on my first book for the last 10 years. It's called \"The Secret Life of Pets\" and it is about a man named John who has just finished his second year at college. He is a very good student and he wants to be a writer. He also wants to write a novel. So, I decided to start writing this book.\n",
            "I started with a story in the middle of the night and then I wrote it down. I was so excited that I had\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Generate Python samples with pretrained general model"
      ],
      "metadata": {
        "id": "VXZCXsbB7oXh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt =  \"def find_max(numbers):\""
      ],
      "metadata": {
        "id": "dYR6tUbt7RXt"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = tiny_general_tokenizer(prompt, return_tensors=\"pt\")"
      ],
      "metadata": {
        "id": "Cw7e3F-_7rfo"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TextStreamer\n",
        "streamer = TextStreamer(\n",
        "    tiny_general_tokenizer,\n",
        "    skip_prompt=True, # If you set to false, the model will first return the prompt and then the generated text\n",
        "    skip_special_tokens=True\n",
        ")"
      ],
      "metadata": {
        "id": "SXYgc__Z7ufZ"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "outputs = tiny_general_model.generate(\n",
        "    **inputs,\n",
        "    streamer=streamer,\n",
        "    use_cache=True,\n",
        "    max_new_tokens=128,\n",
        "    do_sample=False,\n",
        "    temperature=0.0,\n",
        "    repetition_penalty=1.1\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IW9_g33S7wh1",
        "outputId": "41fd832c-a4e3-4c5b-ad38-9ccfe8c28a7d"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "       \"\"\"\n",
            "       Returns the number of times a user has been added to the list.\n",
            "       \"\"\"\n",
            "       return num_users() + 1\n",
            "\n",
            "   def get_user_id(self, id):\n",
            "       \"\"\"\n",
            "       Returns the number of users that have been added to the list.\n",
            "       \"\"\"\n",
            "       return len(self.get_users())\n",
            "\n",
            "   def get_user_name(self, name):\n",
            "       \"\"\"\n",
            "       Returns the name of the user that has been added to the list.\n",
            "       \"\"\"\n",
            "       return self.get_user_name(name)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Generate Python samples with finetuned Python model"
      ],
      "metadata": {
        "id": "Vwa2SaDo7-9w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_path_or_name = \"upstage/TinySolar-248m-4k-code-instruct\""
      ],
      "metadata": {
        "id": "kSzoUnxx8CN5"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tiny_finetuned_model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_path_or_name,\n",
        "    device_map=\"cpu\",\n",
        "    torch_dtype=torch.bfloat16,\n",
        ")\n",
        "\n",
        "tiny_finetuned_tokenizer = AutoTokenizer.from_pretrained(\n",
        "    model_path_or_name\n",
        ")"
      ],
      "metadata": {
        "id": "A5Nqwmdb8MN9"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt =  \"def find_max(numbers):\""
      ],
      "metadata": {
        "id": "b8MIWxBK7y5N"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TextStreamer\n",
        "streamer = TextStreamer(\n",
        "    tiny_general_tokenizer,\n",
        "    skip_prompt=True, # If you set to false, the model will first return the prompt and then the generated text\n",
        "    skip_special_tokens=True\n",
        ")"
      ],
      "metadata": {
        "id": "plran7bm8JlI"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "outputs = tiny_general_model.generate(\n",
        "    **inputs,\n",
        "    streamer=streamer,\n",
        "    use_cache=True,\n",
        "    max_new_tokens=128,\n",
        "    do_sample=False,\n",
        "    temperature=0.0,\n",
        "    repetition_penalty=1.1\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tIZaBITo8ZFj",
        "outputId": "27e12001-12a5-45a3-cb82-e19e3106d680"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "       \"\"\"\n",
            "       Returns the number of times a user has been added to the list.\n",
            "       \"\"\"\n",
            "       return num_users() + 1\n",
            "\n",
            "   def get_user_id(self, id):\n",
            "       \"\"\"\n",
            "       Returns the number of users that have been added to the list.\n",
            "       \"\"\"\n",
            "       return len(self.get_users())\n",
            "\n",
            "   def get_user_name(self, name):\n",
            "       \"\"\"\n",
            "       Returns the name of the user that has been added to the list.\n",
            "       \"\"\"\n",
            "       return self.get_user_name(name)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Generate Python samples with pretrained Python model\n"
      ],
      "metadata": {
        "id": "SLdcZLi69fj8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_path_or_name = \"upstage/TinySolar-248m-4k-py\""
      ],
      "metadata": {
        "id": "0Oa4XNXn9dkJ"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tiny_custom_model=AutoModelForCausalLM.from_pretrained(\n",
        "    model_path_or_name,\n",
        "    device_map='cpu',\n",
        "    torch_dtype=torch.bfloat16,\n",
        ")\n",
        "\n",
        "tiny_custom_tokenizer=AutoTokenizer.from_pretrained(\n",
        "    model_path_or_name\n",
        ")"
      ],
      "metadata": {
        "id": "8ucVyq4p9ptt"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"def find_max(numbers):\"\n"
      ],
      "metadata": {
        "id": "qOLSgx2W-TJh"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = tiny_custom_tokenizer(\n",
        "    prompt, return_tensors=\"pt\"\n",
        ").to(tiny_custom_model.device)"
      ],
      "metadata": {
        "id": "sgZROmCj-i6K"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "streamer=TextStreamer(\n",
        "    tiny_custom_tokenizer,\n",
        "    skip_prompt=True,\n",
        "    skip_special_tokens=True\n",
        ")"
      ],
      "metadata": {
        "id": "HCo1J8Xa-lu-"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "outputs=tiny_custom_model.generate(\n",
        "    **inputs, streamer=streamer,\n",
        "    use_cache=True,\n",
        "    max_new_tokens=128,\n",
        "    do_sample=False,\n",
        "    repetition_penalty=1.1\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TFuPxsNO-qVJ",
        "outputId": "122b5235-fb53-43ea-f02b-eacdbc74ed43"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "   \"\"\"Find the maximum number of numbers in a list.\"\"\"\n",
            "   max = 0\n",
            "   for num in numbers:\n",
            "       if num > max:\n",
            "           max = num\n",
            "   return max\n",
            "\n",
            "\n",
            "def get_min_max(numbers, min_value=1):\n",
            "   \"\"\"Get the minimum value of a list.\"\"\"\n",
            "   min_value = min_value or 1\n",
            "   for num in numbers:\n",
            "       if num < min_value:\n",
            "           min_value = num\n",
            "   return min_value\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test Generated Code"
      ],
      "metadata": {
        "id": "kE4P4aZS_R9k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_min_max(numbers, min_value=1):\n",
        "   \"\"\"Get the minimum value of a list.\"\"\"\n",
        "   min_value = min_value or 1\n",
        "   for num in numbers:\n",
        "       if num < min_value:\n",
        "           min_value = num\n",
        "   return min_value"
      ],
      "metadata": {
        "id": "3qrIlNU__QPw"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "get_min_max([1,5,4,8,6,2,4,7])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eMrzVpTm_ApV",
        "outputId": "554a1a25-f83f-4236-c35d-85ba4de4d26a"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import nbformat\n",
        "from google.colab import _message\n",
        "\n",
        "# Get notebook JSON from Colab\n",
        "nb_json = _message.blocking_request('get_ipynb')['ipynb']\n",
        "\n",
        "# Convert to nbformat object\n",
        "nb = nbformat.from_dict(nb_json)\n",
        "\n",
        "# Remove widget metadata if present\n",
        "if \"widgets\" in nb.metadata:\n",
        "    nb.metadata.pop(\"widgets\")\n",
        "\n",
        "# Push the cleaned notebook back into Colab memory\n",
        "cleaned = nbformat.writes(nb)\n",
        "_message.blocking_request('set_ipynb', {'ipynb': json.loads(cleaned)})\n",
        "\n",
        "print(\"✅ Cleaned! Now use File → Save a copy to GitHub again.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pQ7212kNjYzn",
        "outputId": "5e9dd666-cd87-4d5b-9003-30dcc3bacb38"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Cleaned! Now use File → Save a copy to GitHub again.\n"
          ]
        }
      ]
    }
  ]
}