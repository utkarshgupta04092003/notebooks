{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN92/0Br7r1EkiO6kKqmrrI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/utkarshgupta04092003/notebooks/blob/main/pretrain-llms/2_Train%2C_Test_and_Evaluate_Pretrain_Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ignore warnings\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "NSByO4Zc4K6e"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set a seed value for reproducibility\n",
        "import torch\n",
        "def fix_torch_seed(seed=32):\n",
        "  torch.manual_seed(seed)\n",
        "  torch.cuda.manual_seed(seed)\n",
        "  torch.backends.cudnn.deterministic = True\n",
        "  torch.backends.cudnn.benchmark = False\n",
        "\n",
        "fix_torch_seed()"
      ],
      "metadata": {
        "id": "2LjIHBIY7h7K"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Model Configuration"
      ],
      "metadata": {
        "id": "ywzTGY5i76St"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import LlamaConfig\n",
        "config = LlamaConfig()\n",
        "config"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2d872Pw873mc",
        "outputId": "20e2e40c-738f-4aee-ebc6-a9cfd25ca326"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LlamaConfig {\n",
              "  \"attention_bias\": false,\n",
              "  \"attention_dropout\": 0.0,\n",
              "  \"bos_token_id\": 1,\n",
              "  \"eos_token_id\": 2,\n",
              "  \"head_dim\": 128,\n",
              "  \"hidden_act\": \"silu\",\n",
              "  \"hidden_size\": 4096,\n",
              "  \"initializer_range\": 0.02,\n",
              "  \"intermediate_size\": 11008,\n",
              "  \"max_position_embeddings\": 2048,\n",
              "  \"mlp_bias\": false,\n",
              "  \"model_type\": \"llama\",\n",
              "  \"num_attention_heads\": 32,\n",
              "  \"num_hidden_layers\": 32,\n",
              "  \"num_key_value_heads\": 32,\n",
              "  \"pretraining_tp\": 1,\n",
              "  \"rms_norm_eps\": 1e-06,\n",
              "  \"rope_scaling\": null,\n",
              "  \"rope_theta\": 10000.0,\n",
              "  \"tie_word_embeddings\": false,\n",
              "  \"transformers_version\": \"4.56.2\",\n",
              "  \"use_cache\": true,\n",
              "  \"vocab_size\": 32000\n",
              "}"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Adjust parameters to reduce model size\n",
        "config.vocab_size = 32000              # must match tokenizer\n",
        "config.hidden_size = 1024              # embedding dimension\n",
        "config.intermediate_size = 4096        # ~4x hidden_size\n",
        "config.num_hidden_layers = 12          # number of transformer blocks\n",
        "config.num_attention_heads = 8         # must divide hidden_size\n",
        "config.num_key_value_heads = 8         # usually same as attention heads\n",
        "config.max_position_embeddings = 2048  # reduce if needed\n",
        "config.use_cache = False\n",
        "config"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HYvxpK_t9FBR",
        "outputId": "4b7ef9a6-03eb-490c-99a9-8839c8c87739"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LlamaConfig {\n",
              "  \"attention_bias\": false,\n",
              "  \"attention_dropout\": 0.0,\n",
              "  \"bos_token_id\": 1,\n",
              "  \"eos_token_id\": 2,\n",
              "  \"head_dim\": 128,\n",
              "  \"hidden_act\": \"silu\",\n",
              "  \"hidden_size\": 1024,\n",
              "  \"initializer_range\": 0.02,\n",
              "  \"intermediate_size\": 4096,\n",
              "  \"max_position_embeddings\": 2048,\n",
              "  \"mlp_bias\": false,\n",
              "  \"model_type\": \"llama\",\n",
              "  \"num_attention_heads\": 8,\n",
              "  \"num_hidden_layers\": 12,\n",
              "  \"num_key_value_heads\": 8,\n",
              "  \"pretraining_tp\": 1,\n",
              "  \"rms_norm_eps\": 1e-06,\n",
              "  \"rope_scaling\": null,\n",
              "  \"rope_theta\": 10000.0,\n",
              "  \"tie_word_embeddings\": false,\n",
              "  \"transformers_version\": \"4.56.2\",\n",
              "  \"use_cache\": false,\n",
              "  \"vocab_size\": 32000\n",
              "}"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Weight Initialization"
      ],
      "metadata": {
        "id": "gn_Ty0PX-JPr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.1 Random weight initialization"
      ],
      "metadata": {
        "id": "Mk134nub-MWi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import LlamaForCausalLM\n",
        "model = LlamaForCausalLM(config)\n",
        "model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I834S36Q91Z8",
        "outputId": "cc22ca82-0cd8-4aa7-d69f-1166e5f24b92"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LlamaForCausalLM(\n",
              "  (model): LlamaModel(\n",
              "    (embed_tokens): Embedding(32000, 1024)\n",
              "    (layers): ModuleList(\n",
              "      (0-11): 12 x LlamaDecoderLayer(\n",
              "        (self_attn): LlamaAttention(\n",
              "          (q_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "          (k_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "          (v_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "          (o_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "        )\n",
              "        (mlp): LlamaMLP(\n",
              "          (gate_proj): Linear(in_features=1024, out_features=4096, bias=False)\n",
              "          (up_proj): Linear(in_features=1024, out_features=4096, bias=False)\n",
              "          (down_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
              "          (act_fn): SiLU()\n",
              "        )\n",
              "        (input_layernorm): LlamaRMSNorm((1024,), eps=1e-06)\n",
              "        (post_attention_layernorm): LlamaRMSNorm((1024,), eps=1e-06)\n",
              "      )\n",
              "    )\n",
              "    (norm): LlamaRMSNorm((1024,), eps=1e-06)\n",
              "    (rotary_emb): LlamaRotaryEmbedding()\n",
              "  )\n",
              "  (lm_head): Linear(in_features=1024, out_features=32000, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the total number of model parameters\n",
        "def print_nparams(model):\n",
        "  nparams = sum(p.numel() for p in  model.parameters())\n",
        "  print(f'Number of parameters: {nparams}')\n",
        "\n",
        "\n",
        "print_nparams(model) # 342385664 => 342M"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Ji8sZOi-bOy",
        "outputId": "e4e6c047-1e46-41fa-9763-237bf887c765"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of parameters: 266888192\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# See the weight of randomly assigned to model\n",
        "layer_name = 'model.layers.0.self_attn.q_proj.weight'\n",
        "for name, param in model.named_parameters():\n",
        "  if name == layer_name:\n",
        "    print(f'First 30 weight of layer: {name}')\n",
        "    print(param.data.view(-1)[:30])\n",
        "    break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6QGb4s_u_qr-",
        "outputId": "53784433-5f8c-4cfa-892e-324ce8a286f3"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First 30 weight of layer: model.layers.0.self_attn.q_proj.weight\n",
            "tensor([-0.0223,  0.0217,  0.0219, -0.0016, -0.0030, -0.0191,  0.0135,  0.0194,\n",
            "         0.0134, -0.0154, -0.0067,  0.0170, -0.0082, -0.0104,  0.0338,  0.0293,\n",
            "         0.0101, -0.0118, -0.0038,  0.0047,  0.0164, -0.0023,  0.0143, -0.0099,\n",
            "        -0.0117,  0.0167,  0.0182,  0.0135,  0.0076,  0.0169])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# what will be output of the model randomly initialized weight, not trained on any data\n",
        "# Load the tokenizer from Upstage Solar, which is compatible with the Llama-2 tokenizer\n",
        "from transformers import LlamaTokenizer\n",
        "model_dir = 'upstage/SOLAR-10.7B-v1.0'\n",
        "tokenizer = LlamaTokenizer.from_pretrained(model_dir)\n",
        "\n",
        "# Run simple interface with prompt\n",
        "from transformers import TextStreamer\n",
        "prompt = 'I am an engineer, i love'\n",
        "\n",
        "inputs = tokenizer(prompt, return_tensors='pt').to(model.device)\n",
        "streamer = TextStreamer(\n",
        "    tokenizer,\n",
        "    skip_prompt=True,\n",
        "    skip_special_tokens=False\n",
        "\n",
        ")\n",
        "\n",
        "outputs = model.generate(\n",
        "    **inputs,\n",
        "    streamer=streamer,\n",
        "    use_cache=True,\n",
        "    max_new_tokens=128,\n",
        "    do_sample=False\n",
        ")"
      ],
      "metadata": {
        "id": "Djg3G9ag_28o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# release the memory to avoid crashing\n",
        "import gc\n",
        "del model\n",
        "del streamer\n",
        "del outputs\n",
        "gc.collect()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nQ-M82qRCALo",
        "outputId": "cf33a4d3-2e41-4f63-ebb4-8c7a3712e446"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "196"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.2. Reuse general pretrained model weights"
      ],
      "metadata": {
        "id": "3wWm4zTmFvCS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the model\n",
        "from transformers import AutoModelForCausalLM\n",
        "\n",
        "model_name_or_path = 'upstage/TinySolar-248m-4k'\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name_or_path, device_map='cpu', torch_dtype=torch.bfloat16)\n",
        "\n",
        "model"
      ],
      "metadata": {
        "id": "cpTIGPGmFKS1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "del model\n",
        "gc.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J2no5Sf2GKep",
        "outputId": "31762be5-c36e-4941-d090-7eb0d6cc3783"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Downsccaling from a general pretrained model\n",
        "Not good for small models"
      ],
      "metadata": {
        "id": "kf5Zu65rGbiq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoConfig\n",
        "\n",
        "model_name_or_path = 'upstage/TinySolar-248m-4k'\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name_or_path, device_map='cpu', torch_dtype=torch.bfloat16)\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)"
      ],
      "metadata": {
        "id": "XRVhSEcJGT9s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check how many layers and parameter does model have\n",
        "print(model)\n",
        "print_nparams(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_awB_hMyHQLE",
        "outputId": "fd06fa6a-650c-4c35-80d1-289ab3c969ef"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LlamaForCausalLM(\n",
            "  (model): LlamaModel(\n",
            "    (embed_tokens): Embedding(32000, 1024)\n",
            "    (layers): ModuleList(\n",
            "      (0-11): 12 x LlamaDecoderLayer(\n",
            "        (self_attn): LlamaAttention(\n",
            "          (q_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
            "          (k_proj): Linear(in_features=1024, out_features=256, bias=False)\n",
            "          (v_proj): Linear(in_features=1024, out_features=256, bias=False)\n",
            "          (o_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
            "        )\n",
            "        (mlp): LlamaMLP(\n",
            "          (gate_proj): Linear(in_features=1024, out_features=4096, bias=False)\n",
            "          (up_proj): Linear(in_features=1024, out_features=4096, bias=False)\n",
            "          (down_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
            "          (act_fn): SiLU()\n",
            "        )\n",
            "        (input_layernorm): LlamaRMSNorm((1024,), eps=1e-06)\n",
            "        (post_attention_layernorm): LlamaRMSNorm((1024,), eps=1e-06)\n",
            "      )\n",
            "    )\n",
            "    (norm): LlamaRMSNorm((1024,), eps=1e-06)\n",
            "    (rotary_emb): LlamaRotaryEmbedding()\n",
            "  )\n",
            "  (lm_head): Linear(in_features=1024, out_features=32000, bias=False)\n",
            ")\n",
            "Number of parameters: 248013824\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Number of parameters: 248013824 => 248M\n"
      ],
      "metadata": {
        "id": "CEGPrn0pIifH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create smaller model by remove hidden layers\n",
        "layers = model.model.layers\n",
        "model.model.layers = layers[:5] + layers[-5:]\n",
        "\n",
        "config = AutoConfig.from_pretrained(\n",
        "    model_name_or_path,\n",
        "    num_hidden_layers = len(model.model.layers)\n",
        ")\n",
        "\n",
        "model.config = config\n",
        "print_nparams(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v_Cuz6VwHdNw",
        "outputId": "86d648b4-6cfd-498d-ebcb-6656091551d2"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of parameters: 217601024\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Number of parameters: 217601024 => 217M\n"
      ],
      "metadata": {
        "id": "yTBidMhTInI3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Depth Upscaling from a general pretrained model\n"
      ],
      "metadata": {
        "id": "aOtukR72Iv8v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "config = LlamaConfig(\n",
        "    num_hidden_layers=16,\n",
        "    hidden_size=1024,\n",
        "    intermediate_size=4096,\n",
        "    num_attention_heads=32,\n",
        "    num_key_value_heads=8,\n",
        "    torch_dtype = 'bfloat16',\n",
        "    use_cache=False,\n",
        ")\n",
        "config"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nyh17_c4H-kd",
        "outputId": "1024bbbe-281a-4a39-bcba-b787bfb8215c"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LlamaConfig {\n",
              "  \"attention_bias\": false,\n",
              "  \"attention_dropout\": 0.0,\n",
              "  \"bos_token_id\": 1,\n",
              "  \"dtype\": \"bfloat16\",\n",
              "  \"eos_token_id\": 2,\n",
              "  \"head_dim\": 32,\n",
              "  \"hidden_act\": \"silu\",\n",
              "  \"hidden_size\": 1024,\n",
              "  \"initializer_range\": 0.02,\n",
              "  \"intermediate_size\": 4096,\n",
              "  \"max_position_embeddings\": 2048,\n",
              "  \"mlp_bias\": false,\n",
              "  \"model_type\": \"llama\",\n",
              "  \"num_attention_heads\": 32,\n",
              "  \"num_hidden_layers\": 16,\n",
              "  \"num_key_value_heads\": 8,\n",
              "  \"pretraining_tp\": 1,\n",
              "  \"rms_norm_eps\": 1e-06,\n",
              "  \"rope_scaling\": null,\n",
              "  \"rope_theta\": 10000.0,\n",
              "  \"tie_word_embeddings\": false,\n",
              "  \"transformers_version\": \"4.56.2\",\n",
              "  \"use_cache\": false,\n",
              "  \"vocab_size\": 32000\n",
              "}"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = LlamaForCausalLM(config)\n",
        "model = model.to(dtype=torch.bfloat16)\n",
        "print_nparams(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AlZaS2d4JRim",
        "outputId": "3a4c4d19-8db8-4bb1-c3b6-1771280ebf16"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of parameters: 308839424\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Number of parameters: 308839424 => 308M"
      ],
      "metadata": {
        "id": "x_FNfsDdJkLq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_name_or_path = 'upstage/TinySolar-248m-4k'\n",
        "pretrained_model = AutoModelForCausalLM.from_pretrained(model_name_or_path, device_map='cpu', torch_dtype=torch.bfloat16)\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n",
        "print_nparams(pretrained_model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w1Z3JCsGJij6",
        "outputId": "cd04653f-ae85-41d3-fdfe-ffe2739ad6c1"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of parameters: 248013824\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Number of parameters: 248013824 => 248M"
      ],
      "metadata": {
        "id": "cMA0dpPkKDbA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from copy import deepcopy\n",
        "\n",
        "model.model.layers = deepcopy(pretrained_model.model.layers[:-4]) + deepcopy(pretrained_model.model.layers[4:])\n",
        "model.model.embed_tokens = deepcopy(pretrained_model.model.embed_tokens)\n",
        "model.lm_head = deepcopy(pretrained_model.lm_head)\n",
        "\n",
        "print(model.config)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ATMqvl0dJcLt",
        "outputId": "ad0f8316-e955-4899-8896-22446d7a85d7"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LlamaConfig {\n",
            "  \"attention_bias\": false,\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 1,\n",
            "  \"dtype\": \"bfloat16\",\n",
            "  \"eos_token_id\": 2,\n",
            "  \"head_dim\": 32,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"max_position_embeddings\": 2048,\n",
            "  \"mlp_bias\": false,\n",
            "  \"model_type\": \"llama\",\n",
            "  \"num_attention_heads\": 32,\n",
            "  \"num_hidden_layers\": 16,\n",
            "  \"num_key_value_heads\": 8,\n",
            "  \"pretraining_tp\": 1,\n",
            "  \"rms_norm_eps\": 1e-06,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 10000.0,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"transformers_version\": \"4.56.2\",\n",
            "  \"use_cache\": false,\n",
            "  \"vocab_size\": 32000\n",
            "}\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print_nparams(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4a9NK_s5KrGp",
        "outputId": "d976d379-f8fd-4c0b-e808-5eb8c51baaca"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of parameters: 308839424\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Number of parameters: 308839424 => 308M (by deepcopy 2 248M models)"
      ],
      "metadata": {
        "id": "JesLKAdyKutr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = 'I am an engineer, i love'\n",
        "\n",
        "inputs = tokenizer(prompt, return_tensors='pt').to(model.device)\n",
        "streamer = TextStreamer(\n",
        "    tokenizer,\n",
        "    skip_prompt=True,\n",
        "    skip_special_tokens=False\n",
        "\n",
        ")\n",
        "\n",
        "outputs = model.generate(\n",
        "    **inputs,\n",
        "    streamer=streamer,\n",
        "    use_cache=True,\n",
        "    max_new_tokens=128,\n",
        "    do_sample=False\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q9BFJSWoKtCr",
        "outputId": "6902a972-ac8d-4d71-b27d-92233ded5f88"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "to use the word \"miscellaneous\" to describe the \"miscellaneous\" of a website.\n",
            "I have been a long-time user of the site, and I have been a fan of it. I have been a fan of the site, and I have been a fan of the site's history. I have been a fan of the site's history, and I have been a fan of the site's history.\n",
            "I have been a fan of the site's history, and I have been a fan of the site's history. I have been a fan of the site's\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model is initialized, but since it hasn't been pretrained, its responses aren't fluent. That's why pretraining is necessary after model preparation"
      ],
      "metadata": {
        "id": "SEBxB6odLOi3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# weight will be optimized at the time of pretraining, save the model for now\n",
        "model.save_pretrained('TinySolar-308m-4k-init')"
      ],
      "metadata": {
        "id": "le_YsblkLivr"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training Model"
      ],
      "metadata": {
        "id": "i7E5Rd27EZDF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Load Model"
      ],
      "metadata": {
        "id": "PGaKOZ3sIV5b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM\n",
        "\n",
        "pretrained_model = AutoModelForCausalLM.from_pretrained(\n",
        "    '/content/TinySolar-308m-4k-init',\n",
        "    device_map='cpu',\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    use_cache=False\n",
        ")\n",
        "pretrained_model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2kaBmayGHy49",
        "outputId": "781a5486-5db0-47eb-d3fe-f5acc78da159"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LlamaForCausalLM(\n",
              "  (model): LlamaModel(\n",
              "    (embed_tokens): Embedding(32000, 1024)\n",
              "    (layers): ModuleList(\n",
              "      (0-15): 16 x LlamaDecoderLayer(\n",
              "        (self_attn): LlamaAttention(\n",
              "          (q_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "          (k_proj): Linear(in_features=1024, out_features=256, bias=False)\n",
              "          (v_proj): Linear(in_features=1024, out_features=256, bias=False)\n",
              "          (o_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "        )\n",
              "        (mlp): LlamaMLP(\n",
              "          (gate_proj): Linear(in_features=1024, out_features=4096, bias=False)\n",
              "          (up_proj): Linear(in_features=1024, out_features=4096, bias=False)\n",
              "          (down_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
              "          (act_fn): SiLU()\n",
              "        )\n",
              "        (input_layernorm): LlamaRMSNorm((1024,), eps=1e-06)\n",
              "        (post_attention_layernorm): LlamaRMSNorm((1024,), eps=1e-06)\n",
              "      )\n",
              "    )\n",
              "    (norm): LlamaRMSNorm((1024,), eps=1e-06)\n",
              "    (rotary_emb): LlamaRotaryEmbedding()\n",
              "  )\n",
              "  (lm_head): Linear(in_features=1024, out_features=32000, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Load Dataset"
      ],
      "metadata": {
        "id": "iU-aSU5xIYgP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import datasets\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class CustomDataset(Dataset):\n",
        "  def __init__(self, args, split='train'):\n",
        "    self.args = args\n",
        "    self.dataset = datasets.load_dataset(\n",
        "        'parquet',\n",
        "        data_files=args.dataset_name,\n",
        "        split=split\n",
        "    )\n",
        "  def __len__(self):\n",
        "    return len(self.dataset)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    # convert the lists to a LongTensor for Pytorch\n",
        "    input_ids = torch.LongTensor(self.dataset[idx]['input_ids'])\n",
        "    labels = torch.LongTensor(self.dataset[idx]['input_ids'])\n",
        "    # return the sample as a dictionary\n",
        "    return {'input_ids': input_ids, 'labels': labels}"
      ],
      "metadata": {
        "id": "nS1SVtzOIDNq"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Configure Trainng Arguments"
      ],
      "metadata": {
        "id": "zzqWeqjHJdvM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from dataclasses import dataclass, field\n",
        "import transformers\n",
        "\n",
        "@dataclass\n",
        "class CustomArguments(transformers.TrainingArguments):\n",
        "  # Dataset configuration\n",
        "  dataset_name: str = field(default='/content/packged_pretrain_dataset.parquet')\n",
        "  num_proc: int = field(default=1)\n",
        "  max_seq_length: int = field(default = 32)\n",
        "\n",
        "  #core training configurations\n",
        "  optim: str = field(default='adamw_torch') # trial and test which optimizer will be good for your usecases\n",
        "  max_steps: int = field(default=30)\n",
        "  per_device_train_batch_size: int = field(default=2)\n",
        "\n",
        "  # Other training configurations\n",
        "  seed: int = field(default=0)\n",
        "  learning_rate: float = field(default=1e-4)\n",
        "  weight_decay: float = field(default=0)\n",
        "  warmup_steps: int = field(default=10)\n",
        "  lr_scheduler_type: str = field(default='linear')\n",
        "  gradient_checkpointing: bool = field(default=True)\n",
        "  dataloader_num_workers: int = field(default=2)\n",
        "  bf16: bool = field(default=False)\n",
        "  gradient_accumulation_steps: int = field(default=1)\n",
        "\n",
        "  # Logging configuration\n",
        "  logging_steps: int = field(default=3)\n",
        "  report_to: str = field(default='none')\n",
        "\n",
        "  # # Save configuration (save intermediate checkpoint of the model)\n",
        "  # save_strategy: str = field(default='steps')\n",
        "  # save_steps: int = field(default=3)\n",
        "  # save_total_limit: int = field(default=2)\n"
      ],
      "metadata": {
        "id": "heDcRPGUIRHf"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "parser = transformers.HfArgumentParser(CustomArguments)\n",
        "args,  = parser.parse_args_into_dataclasses(\n",
        "    args=['--output_dir', 'output', '--fp16']\n",
        ")"
      ],
      "metadata": {
        "id": "J1zTDW5bKWhs"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = CustomDataset(args)"
      ],
      "metadata": {
        "id": "ZjLhM5TNMcNW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Input shape: \", train_dataset[0]['input_ids'].shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sLl00GaHNQm1",
        "outputId": "999bf9ae-7110-46c9-b2da-9319081e56c6"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input shape:  torch.Size([32])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Run the trainer and monitor the loss"
      ],
      "metadata": {
        "id": "PeZOSQs9N_Ub"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Trainer, TrainingArguments, TrainerCallback\n",
        "\n",
        "# Define a custom callback to log the loss value\n",
        "class LossLoggingCallback(TrainerCallback):\n",
        "  def on_log(self, args, state, control, logs=None, **kwargs):\n",
        "    if logs is not None:\n",
        "      self.logs.append(logs)\n",
        "\n",
        "  def __init__(self):\n",
        "    self.logs = []\n",
        "\n",
        "# Initialize the callback\n",
        "loss_logging_callback = LossLoggingCallback()"
      ],
      "metadata": {
        "id": "iDvHhiYbNYAW"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = Trainer(\n",
        "    model = pretrained_model,\n",
        "    args=args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=None,\n",
        "    callbacks=[loss_logging_callback]\n",
        ")\n",
        "trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "MzFVsZdxOhyl",
        "outputId": "5af2ef5c-7794-40ab-e45c-a9848c5077ca"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='30' max='30' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [30/30 47:49, Epoch 0/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>5.331100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>5.163100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>4.877500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>5.031900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15</td>\n",
              "      <td>5.001200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18</td>\n",
              "      <td>4.765000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>21</td>\n",
              "      <td>5.257800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>24</td>\n",
              "      <td>4.941400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>27</td>\n",
              "      <td>4.694600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>4.730100</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=30, training_loss=4.979365571339925, metrics={'train_runtime': 3050.574, 'train_samples_per_second': 0.02, 'train_steps_per_second': 0.01, 'total_flos': 3180342804480.0, 'train_loss': 4.979365571339925, 'epoch': 0.0001575274491580158})"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "TrainOutput(global_step=30, training_loss=4.979365571339925, metrics={'train_runtime': 3050.574, 'train_samples_per_second': 0.02, 'train_steps_per_second': 0.01, 'total_flos': 3180342804480.0, 'train_loss': 4.979365571339925, 'epoch': 0.0001575274491580158})"
      ],
      "metadata": {
        "id": "sXCdtV3g0FFm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the model training checkpoint\n",
        "trainer.save_model('output/checkpoint-10000')"
      ],
      "metadata": {
        "id": "k-lQ5NQVFmu3"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Saving configuration\n",
        "# save_strategy:str = field(default='steps')\n",
        "# save_steps: int = field(default=3)\n",
        "# save_total_limit: int = field(default=2)"
      ],
      "metadata": {
        "id": "uxIAjcGNO6k6"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, TextStreamer\n",
        "model_name_or_path = 'upstage/TinySolar-248m-4k'\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)"
      ],
      "metadata": {
        "id": "S-Ou5hOc1ctV"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, TextStreamer, AutoModelForCausalLM\n",
        "import torch\n",
        "\n",
        "model_name_or_path = '/content/output/checkpoint-10000'\n",
        "model2 = AutoModelForCausalLM.from_pretrained(model_name_or_path, device_map='cpu', torch_dtype=torch.bfloat16)\n",
        "#"
      ],
      "metadata": {
        "id": "n5YUC_MF1nTo"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Lets look at this output\n",
        "prompt = \"I am an engineer, i love\"\n",
        "inputs = tokenizer(prompt, return_tensors='pt').to(model2.device)\n",
        "\n",
        "streamer = TextStreamer(\n",
        "    tokenizer,\n",
        "    skip_prompt=True,\n",
        "    skip_special_tokens=False\n",
        ")\n",
        "\n",
        "outputs = model2.generate(\n",
        "    **inputs,\n",
        "    streamer=streamer,\n",
        "    use_cache=True,\n",
        "    max_new_tokens=64,\n",
        "    do_sample=True,\n",
        "    temperature=1.0\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "okxqhICQ2Bif",
        "outputId": "fef6c3e1-8824-4740-aa46-c8962747d19e"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "my family , which turned to a person. During the year 20018 , the first album was released to Music and Music World . The third decade and album album and band started to come 2006 to be the first album album ever released , a first album that I used to play a\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Evaluation"
      ],
      "metadata": {
        "id": "-43e-Ndp4gCD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# we will use open popular open source evaluation library called lm-evaluation-harness of eluetherAI\n",
        "# harness serves many, many tasks for evaluation, among them we will choose to evaluate tiny solar on the `TruthfulQA MC2` task.\n",
        "# The MC2 task comprised of multiple choice questions developed by the University of Oxford and OpenAI, and is one of the evaluation task includedin the HuggingFace open LLM leaderboard.\n",
        "# MC2 task works as follow: Give an question and multiple true false reference answers, the score is the normalized  total probability assigned to the set of true answers.\n",
        "# We will run this evaluation on CPU, as always and will run only 5 examples, so that we can end our evaluation within ten minutes.\n",
        "\n",
        "!pip install -U git+http://github.com/EleutherAI/lm-evaluation-harness"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hN33Ei5I4jKQ",
        "outputId": "34749e0c-1b09-44c9-a509-d6abd4c62faa"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+http://github.com/EleutherAI/lm-evaluation-harness\n",
            "  Cloning http://github.com/EleutherAI/lm-evaluation-harness to /tmp/pip-req-build-kpmd67cf\n",
            "  Running command git clone --filter=blob:none --quiet http://github.com/EleutherAI/lm-evaluation-harness /tmp/pip-req-build-kpmd67cf\n",
            "  warning: redirecting to https://github.com/EleutherAI/lm-evaluation-harness/\n",
            "  Resolved http://github.com/EleutherAI/lm-evaluation-harness to commit c0fc717240032aec738c3199ae344887b5f34c23\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: accelerate>=0.26.0 in /usr/local/lib/python3.12/dist-packages (from lm_eval==0.4.9.1) (1.10.1)\n",
            "Requirement already satisfied: evaluate in /usr/local/lib/python3.12/dist-packages (from lm_eval==0.4.9.1) (0.4.6)\n",
            "Requirement already satisfied: datasets>=2.16.0 in /usr/local/lib/python3.12/dist-packages (from lm_eval==0.4.9.1) (4.0.0)\n",
            "Requirement already satisfied: jsonlines in /usr/local/lib/python3.12/dist-packages (from lm_eval==0.4.9.1) (4.0.0)\n",
            "Requirement already satisfied: numexpr in /usr/local/lib/python3.12/dist-packages (from lm_eval==0.4.9.1) (2.13.1)\n",
            "Requirement already satisfied: peft>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from lm_eval==0.4.9.1) (0.17.1)\n",
            "Requirement already satisfied: pybind11>=2.6.2 in /usr/local/lib/python3.12/dist-packages (from lm_eval==0.4.9.1) (3.0.1)\n",
            "Requirement already satisfied: pytablewriter in /usr/local/lib/python3.12/dist-packages (from lm_eval==0.4.9.1) (1.2.1)\n",
            "Requirement already satisfied: rouge-score>=0.0.4 in /usr/local/lib/python3.12/dist-packages (from lm_eval==0.4.9.1) (0.1.2)\n",
            "Requirement already satisfied: sacrebleu>=1.5.0 in /usr/local/lib/python3.12/dist-packages (from lm_eval==0.4.9.1) (2.5.1)\n",
            "Requirement already satisfied: scikit-learn>=0.24.1 in /usr/local/lib/python3.12/dist-packages (from lm_eval==0.4.9.1) (1.6.1)\n",
            "Requirement already satisfied: sqlitedict in /usr/local/lib/python3.12/dist-packages (from lm_eval==0.4.9.1) (2.1.0)\n",
            "Requirement already satisfied: torch>=1.8 in /usr/local/lib/python3.12/dist-packages (from lm_eval==0.4.9.1) (2.8.0+cu126)\n",
            "Requirement already satisfied: tqdm-multiprocess in /usr/local/lib/python3.12/dist-packages (from lm_eval==0.4.9.1) (0.0.11)\n",
            "Requirement already satisfied: transformers>=4.1 in /usr/local/lib/python3.12/dist-packages (from lm_eval==0.4.9.1) (4.56.2)\n",
            "Requirement already satisfied: zstandard in /usr/local/lib/python3.12/dist-packages (from lm_eval==0.4.9.1) (0.25.0)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.12/dist-packages (from lm_eval==0.4.9.1) (0.3.8)\n",
            "Requirement already satisfied: word2number in /usr/local/lib/python3.12/dist-packages (from lm_eval==0.4.9.1) (1.1)\n",
            "Requirement already satisfied: more_itertools in /usr/local/lib/python3.12/dist-packages (from lm_eval==0.4.9.1) (10.8.0)\n",
            "Requirement already satisfied: numpy<3.0.0,>=1.17 in /usr/local/lib/python3.12/dist-packages (from accelerate>=0.26.0->lm_eval==0.4.9.1) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from accelerate>=0.26.0->lm_eval==0.4.9.1) (25.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate>=0.26.0->lm_eval==0.4.9.1) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from accelerate>=0.26.0->lm_eval==0.4.9.1) (6.0.3)\n",
            "Requirement already satisfied: huggingface_hub>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from accelerate>=0.26.0->lm_eval==0.4.9.1) (0.35.3)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from accelerate>=0.26.0->lm_eval==0.4.9.1) (0.6.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets>=2.16.0->lm_eval==0.4.9.1) (3.19.1)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets>=2.16.0->lm_eval==0.4.9.1) (18.1.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets>=2.16.0->lm_eval==0.4.9.1) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.12/dist-packages (from datasets>=2.16.0->lm_eval==0.4.9.1) (2.32.4)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.12/dist-packages (from datasets>=2.16.0->lm_eval==0.4.9.1) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets>=2.16.0->lm_eval==0.4.9.1) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets>=2.16.0->lm_eval==0.4.9.1) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.16.0->lm_eval==0.4.9.1) (2025.3.0)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.12/dist-packages (from rouge-score>=0.0.4->lm_eval==0.4.9.1) (1.4.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (from rouge-score>=0.0.4->lm_eval==0.4.9.1) (3.9.1)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.12/dist-packages (from rouge-score>=0.0.4->lm_eval==0.4.9.1) (1.17.0)\n",
            "Requirement already satisfied: portalocker in /usr/local/lib/python3.12/dist-packages (from sacrebleu>=1.5.0->lm_eval==0.4.9.1) (3.2.0)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.12/dist-packages (from sacrebleu>=1.5.0->lm_eval==0.4.9.1) (2024.11.6)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.12/dist-packages (from sacrebleu>=1.5.0->lm_eval==0.4.9.1) (0.9.0)\n",
            "Requirement already satisfied: colorama in /usr/local/lib/python3.12/dist-packages (from sacrebleu>=1.5.0->lm_eval==0.4.9.1) (0.4.6)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.12/dist-packages (from sacrebleu>=1.5.0->lm_eval==0.4.9.1) (5.4.0)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=0.24.1->lm_eval==0.4.9.1) (1.16.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=0.24.1->lm_eval==0.4.9.1) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=0.24.1->lm_eval==0.4.9.1) (3.6.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8->lm_eval==0.4.9.1) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.8->lm_eval==0.4.9.1) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8->lm_eval==0.4.9.1) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=1.8->lm_eval==0.4.9.1) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8->lm_eval==0.4.9.1) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8->lm_eval==0.4.9.1) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8->lm_eval==0.4.9.1) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8->lm_eval==0.4.9.1) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8->lm_eval==0.4.9.1) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8->lm_eval==0.4.9.1) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8->lm_eval==0.4.9.1) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8->lm_eval==0.4.9.1) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8->lm_eval==0.4.9.1) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8->lm_eval==0.4.9.1) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8->lm_eval==0.4.9.1) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8->lm_eval==0.4.9.1) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8->lm_eval==0.4.9.1) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8->lm_eval==0.4.9.1) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8->lm_eval==0.4.9.1) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8->lm_eval==0.4.9.1) (3.4.0)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.1->lm_eval==0.4.9.1) (0.22.1)\n",
            "Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.12/dist-packages (from jsonlines->lm_eval==0.4.9.1) (25.3.0)\n",
            "Requirement already satisfied: DataProperty<2,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from pytablewriter->lm_eval==0.4.9.1) (1.1.0)\n",
            "Requirement already satisfied: mbstrdecoder<2,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from pytablewriter->lm_eval==0.4.9.1) (1.1.4)\n",
            "Requirement already satisfied: pathvalidate<4,>=2.3.0 in /usr/local/lib/python3.12/dist-packages (from pytablewriter->lm_eval==0.4.9.1) (3.3.1)\n",
            "Requirement already satisfied: tabledata<2,>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from pytablewriter->lm_eval==0.4.9.1) (1.3.4)\n",
            "Requirement already satisfied: tcolorpy<1,>=0.0.5 in /usr/local/lib/python3.12/dist-packages (from pytablewriter->lm_eval==0.4.9.1) (0.1.7)\n",
            "Requirement already satisfied: typepy<2,>=1.3.2 in /usr/local/lib/python3.12/dist-packages (from typepy[datetime]<2,>=1.3.2->pytablewriter->lm_eval==0.4.9.1) (1.3.4)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.16.0->lm_eval==0.4.9.1) (3.12.15)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.21.0->accelerate>=0.26.0->lm_eval==0.4.9.1) (1.1.10)\n",
            "Requirement already satisfied: chardet<6,>=3.0.4 in /usr/local/lib/python3.12/dist-packages (from mbstrdecoder<2,>=1.0.0->pytablewriter->lm_eval==0.4.9.1) (5.2.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets>=2.16.0->lm_eval==0.4.9.1) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets>=2.16.0->lm_eval==0.4.9.1) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets>=2.16.0->lm_eval==0.4.9.1) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets>=2.16.0->lm_eval==0.4.9.1) (2025.8.3)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.8->lm_eval==0.4.9.1) (1.3.0)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.8.0 in /usr/local/lib/python3.12/dist-packages (from typepy[datetime]<2,>=1.3.2->pytablewriter->lm_eval==0.4.9.1) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2018.9 in /usr/local/lib/python3.12/dist-packages (from typepy[datetime]<2,>=1.3.2->pytablewriter->lm_eval==0.4.9.1) (2025.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.8->lm_eval==0.4.9.1) (3.0.3)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk->rouge-score>=0.0.4->lm_eval==0.4.9.1) (8.3.0)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets>=2.16.0->lm_eval==0.4.9.1) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.16.0->lm_eval==0.4.9.1) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.16.0->lm_eval==0.4.9.1) (1.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.16.0->lm_eval==0.4.9.1) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.16.0->lm_eval==0.4.9.1) (6.6.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.16.0->lm_eval==0.4.9.1) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.16.0->lm_eval==0.4.9.1) (1.20.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# this will take quite along time because thelog likelihood is calculated for every candidate\n",
        "!lm_eval --model hf \\\n",
        "  --model_args pretrained=upstage/tinySolar-248m-4k \\\n",
        "  --tasks truthfulqa_mc2 \\\n",
        "  --device cpu \\\n",
        "  --limit 5"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gs7eng0W44St",
        "outputId": "3d0039b9-2155-4db5-d91d-ec7de0548d34"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-10-04 02:43:33.992736: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1759545814.065488   21250 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1759545814.088254   21250 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1759545814.163520   21250 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1759545814.163632   21250 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1759545814.163664   21250 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1759545814.163671   21250 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "WARNING:lm_eval.__main__: --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.\n",
            "INFO:lm_eval.__main__:Selected Tasks: ['truthfulqa_mc2']\n",
            "INFO:lm_eval.evaluator:Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234\n",
            "INFO:lm_eval.evaluator:Initializing hf model, with arguments: {'pretrained': 'upstage/tinySolar-248m-4k'}\n",
            "INFO:lm_eval.models.huggingface:Using device 'cpu'\n",
            "config.json: 100% 687/687 [00:00<00:00, 3.30MB/s]\n",
            "tokenizer_config.json: 100% 966/966 [00:00<00:00, 5.68MB/s]\n",
            "tokenizer.model: 100% 493k/493k [00:00<00:00, 1.40MB/s]\n",
            "tokenizer.json: 1.80MB [00:00, 38.5MB/s]\n",
            "special_tokens_map.json: 100% 414/414 [00:00<00:00, 2.98MB/s]\n",
            "INFO:lm_eval.models.huggingface:Model parallel was set to False, max memory was not set, and device map was set to {'': 'cpu'}\n",
            "`torch_dtype` is deprecated! Use `dtype` instead!\n",
            "model.safetensors: 100% 496M/496M [00:12<00:00, 40.5MB/s]\n",
            "generation_config.json: 100% 111/111 [00:00<00:00, 665kB/s]\n",
            "README.md: 9.59kB [00:00, 25.2MB/s]\n",
            "multiple_choice/validation-00000-of-0000(…): 100% 271k/271k [00:00<00:00, 488kB/s] \n",
            "Generating validation split: 100% 817/817 [00:00<00:00, 18878.38 examples/s]\n",
            "INFO:lm_eval.api.task:Building contexts for truthfulqa_mc2 on rank 0...\n",
            "100% 5/5 [00:00<00:00, 594.90it/s]\n",
            "INFO:lm_eval.evaluator:Running loglikelihood requests\n",
            "Running loglikelihood requests: 100% 33/33 [05:06<00:00,  9.30s/it]\n",
            "fatal: not a git repository (or any of the parent directories): .git\n",
            "INFO:lm_eval.loggers.evaluation_tracker:Output path not provided, skipping saving results aggregated\n",
            "hf (pretrained=upstage/tinySolar-248m-4k), gen_kwargs: (None), limit: 5.0, num_fewshot: None, batch_size: 1\n",
            "|    Tasks     |Version|Filter|n-shot|Metric|   |Value |   |Stderr|\n",
            "|--------------|------:|------|-----:|------|---|-----:|---|-----:|\n",
            "|truthfulqa_mc2|      3|none  |     0|acc   |↑  |0.4007|±  |0.2447|\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You get score as around 0.4 and its unfair to compare this with another model that has billions of parameters."
      ],
      "metadata": {
        "id": "yur1QXJH7VsC"
      }
    }
  ]
}