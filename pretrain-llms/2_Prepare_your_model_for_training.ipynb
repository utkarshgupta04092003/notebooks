{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP5Qi4DT+AuLq6OBaZDNNtw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/utkarshgupta04092003/notebooks/blob/main/pretrain-llms/2_Prepare_your_model_for_training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ignore warnings\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "NSByO4Zc4K6e"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set a seed value for reproducibility\n",
        "import torch\n",
        "def fix_torch_seed(seed=32):\n",
        "  torch.manual_seed(seed)\n",
        "  torch.cuda.manual_seed(seed)\n",
        "  torch.backends.cudnn.deterministic = True\n",
        "  torch.backends.cudnn.benchmark = False\n",
        "\n",
        "fix_torch_seed()"
      ],
      "metadata": {
        "id": "2LjIHBIY7h7K"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Model Configuration"
      ],
      "metadata": {
        "id": "ywzTGY5i76St"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import LlamaConfig\n",
        "config = LlamaConfig()\n",
        "config"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2d872Pw873mc",
        "outputId": "e74bcafe-01c4-4bc9-af68-fe5649dbb8a6"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LlamaConfig {\n",
              "  \"attention_bias\": false,\n",
              "  \"attention_dropout\": 0.0,\n",
              "  \"bos_token_id\": 1,\n",
              "  \"eos_token_id\": 2,\n",
              "  \"head_dim\": 128,\n",
              "  \"hidden_act\": \"silu\",\n",
              "  \"hidden_size\": 4096,\n",
              "  \"initializer_range\": 0.02,\n",
              "  \"intermediate_size\": 11008,\n",
              "  \"max_position_embeddings\": 2048,\n",
              "  \"mlp_bias\": false,\n",
              "  \"model_type\": \"llama\",\n",
              "  \"num_attention_heads\": 32,\n",
              "  \"num_hidden_layers\": 32,\n",
              "  \"num_key_value_heads\": 32,\n",
              "  \"pretraining_tp\": 1,\n",
              "  \"rms_norm_eps\": 1e-06,\n",
              "  \"rope_scaling\": null,\n",
              "  \"rope_theta\": 10000.0,\n",
              "  \"tie_word_embeddings\": false,\n",
              "  \"transformers_version\": \"4.56.1\",\n",
              "  \"use_cache\": true,\n",
              "  \"vocab_size\": 32000\n",
              "}"
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Adjust parameters to reduce model size\n",
        "config.vocab_size = 32000              # must match tokenizer\n",
        "config.hidden_size = 1024              # embedding dimension\n",
        "config.intermediate_size = 4096        # ~4x hidden_size\n",
        "config.num_hidden_layers = 12          # number of transformer blocks\n",
        "config.num_attention_heads = 8         # must divide hidden_size\n",
        "config.num_key_value_heads = 8         # usually same as attention heads\n",
        "config.max_position_embeddings = 2048  # reduce if needed\n",
        "config.use_cache = False\n",
        "config"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HYvxpK_t9FBR",
        "outputId": "a8122f17-f15c-4a24-d664-64e82ed7aedd"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LlamaConfig {\n",
              "  \"attention_bias\": false,\n",
              "  \"attention_dropout\": 0.0,\n",
              "  \"bos_token_id\": 1,\n",
              "  \"eos_token_id\": 2,\n",
              "  \"head_dim\": 128,\n",
              "  \"hidden_act\": \"silu\",\n",
              "  \"hidden_size\": 1024,\n",
              "  \"initializer_range\": 0.02,\n",
              "  \"intermediate_size\": 4096,\n",
              "  \"max_position_embeddings\": 2048,\n",
              "  \"mlp_bias\": false,\n",
              "  \"model_type\": \"llama\",\n",
              "  \"num_attention_heads\": 8,\n",
              "  \"num_hidden_layers\": 12,\n",
              "  \"num_key_value_heads\": 8,\n",
              "  \"pretraining_tp\": 1,\n",
              "  \"rms_norm_eps\": 1e-06,\n",
              "  \"rope_scaling\": null,\n",
              "  \"rope_theta\": 10000.0,\n",
              "  \"tie_word_embeddings\": false,\n",
              "  \"transformers_version\": \"4.56.1\",\n",
              "  \"use_cache\": false,\n",
              "  \"vocab_size\": 32000\n",
              "}"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Weight Initialization"
      ],
      "metadata": {
        "id": "gn_Ty0PX-JPr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.1 Random weight initialization"
      ],
      "metadata": {
        "id": "Mk134nub-MWi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import LlamaForCausalLM\n",
        "model = LlamaForCausalLM(config)\n",
        "model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I834S36Q91Z8",
        "outputId": "525fc1dd-d76b-4f6b-b0aa-6a0be8652743"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LlamaForCausalLM(\n",
              "  (model): LlamaModel(\n",
              "    (embed_tokens): Embedding(32000, 1024)\n",
              "    (layers): ModuleList(\n",
              "      (0-11): 12 x LlamaDecoderLayer(\n",
              "        (self_attn): LlamaAttention(\n",
              "          (q_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "          (k_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "          (v_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "          (o_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "        )\n",
              "        (mlp): LlamaMLP(\n",
              "          (gate_proj): Linear(in_features=1024, out_features=4096, bias=False)\n",
              "          (up_proj): Linear(in_features=1024, out_features=4096, bias=False)\n",
              "          (down_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
              "          (act_fn): SiLU()\n",
              "        )\n",
              "        (input_layernorm): LlamaRMSNorm((1024,), eps=1e-06)\n",
              "        (post_attention_layernorm): LlamaRMSNorm((1024,), eps=1e-06)\n",
              "      )\n",
              "    )\n",
              "    (norm): LlamaRMSNorm((1024,), eps=1e-06)\n",
              "    (rotary_emb): LlamaRotaryEmbedding()\n",
              "  )\n",
              "  (lm_head): Linear(in_features=1024, out_features=32000, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the total number of model parameters\n",
        "def print_nparams(model):\n",
        "  nparams = sum(p.numel() for p in  model.parameters())\n",
        "  print(f'Number of parameters: {nparams}')\n",
        "\n",
        "\n",
        "print_nparams(model) # 342385664 => 342M"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Ji8sZOi-bOy",
        "outputId": "7311458d-2073-4970-b98e-ca27c86e6ffb"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of parameters: 266888192\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# See the weight of randomly assigned to model\n",
        "layer_name = 'model.layers.0.self_attn.q_proj.weight'\n",
        "for name, param in model.named_parameters():\n",
        "  if name == layer_name:\n",
        "    print(f'First 30 weight of layer: {name}')\n",
        "    print(param.data.view(-1)[:30])\n",
        "    break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6QGb4s_u_qr-",
        "outputId": "4883fd5f-3439-4697-cca0-20059e8f2748"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First 30 weight of layer: model.layers.0.self_attn.q_proj.weight\n",
            "tensor([ 0.0025,  0.0228,  0.0313,  0.0041,  0.0171,  0.0195,  0.0103, -0.0034,\n",
            "        -0.0058, -0.0161,  0.0032,  0.0046,  0.0168,  0.0275, -0.0451,  0.0009,\n",
            "         0.0161, -0.0152, -0.0301, -0.0183, -0.0229, -0.0115,  0.0013, -0.0297,\n",
            "         0.0235, -0.0275, -0.0398,  0.0039, -0.0172, -0.0306])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# what will be output of the model randomly initialized weight, not trained on any data\n",
        "# Load the tokenizer from Upstage Solar, which is compatible with the Llama-2 tokenizer\n",
        "from transformers import LlamaTokenizer\n",
        "model_dir = 'upstage/SOLAR-10.7B-v1.0'\n",
        "tokenizer = LlamaTokenizer.from_pretrained(model_dir)\n",
        "\n",
        "# Run simple interface with prompt\n",
        "from transformers import TextStreamer\n",
        "prompt = 'I am an engineer, i love'\n",
        "\n",
        "inputs = tokenizer(prompt, return_tensors='pt').to(model.device)\n",
        "streamer = TextStreamer(\n",
        "    tokenizer,\n",
        "    skip_prompt=True,\n",
        "    skip_special_tokens=False\n",
        "\n",
        ")\n",
        "\n",
        "outputs = model.generate(\n",
        "    **inputs,\n",
        "    streamer=streamer,\n",
        "    use_cache=True,\n",
        "    max_new_tokens=128,\n",
        "    do_sample=False\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Djg3G9ag_28o",
        "outputId": "03e935a2-1b8b-4639-cc84-5b3879f337c1"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rupal Product Product Product nodeណ recognisedណ recognisedណ recognisediele recognisedieleiele\u0019iable altered altered altered altered altered altered☼ frozen дивизи frozen frozen frozen sehricher Z frozen sehr sehr comes Z Z frozen sehr comes Zgz comes comes comeseclipsegetNamegetName comesgetName comesgetName comesgetName Integr comesgetName Integr comesgetName IntegrgetName IntegrgetName IntegrgetNameथgetNameथgetNameथgetNameथgetNameथथथथथथgetName IntegrथgetName Integr Integr Integr Integr Integr Integr Integr Integrренmilmilmilmilmilmilmilmilmilmilmilmilmilmilmilmilmilmilmilmilmilਰਰਰਰਰਰਰਰਰਰਰਰਰ\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# release the memory to avoid crashing\n",
        "import gc\n",
        "del model\n",
        "del streamer\n",
        "del outputs\n",
        "gc.collect()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nQ-M82qRCALo",
        "outputId": "bf7f278b-2d9d-4fdc-bc87-2e2883f67bb6"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "610"
            ]
          },
          "metadata": {},
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.2. Reuse general pretrained model weights"
      ],
      "metadata": {
        "id": "3wWm4zTmFvCS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the model\n",
        "from transformers import AutoModelForCausalLM\n",
        "\n",
        "model_name_or_path = 'upstage/TinySolar-248m-4k'\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name_or_path, device_map='cpu', torch_dtype=torch.bfloat16)\n",
        "\n",
        "model"
      ],
      "metadata": {
        "id": "cpTIGPGmFKS1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "del model\n",
        "gc.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J2no5Sf2GKep",
        "outputId": "39b0aed3-14de-4c8d-da36-47ae6b41575c"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "225"
            ]
          },
          "metadata": {},
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Downsccaling from a general pretrained model\n",
        "Not good for small models"
      ],
      "metadata": {
        "id": "kf5Zu65rGbiq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoConfig\n",
        "\n",
        "model_name_or_path = 'upstage/TinySolar-248m-4k'\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name_or_path, device_map='cpu', torch_dtype=torch.bfloat16)\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)"
      ],
      "metadata": {
        "id": "XRVhSEcJGT9s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check how many layers and parameter does model have\n",
        "print(model)\n",
        "print_nparams(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_awB_hMyHQLE",
        "outputId": "22380fde-2a67-48a5-d38d-530448666cb4"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LlamaForCausalLM(\n",
            "  (model): LlamaModel(\n",
            "    (embed_tokens): Embedding(32000, 1024)\n",
            "    (layers): ModuleList(\n",
            "      (0-11): 12 x LlamaDecoderLayer(\n",
            "        (self_attn): LlamaAttention(\n",
            "          (q_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
            "          (k_proj): Linear(in_features=1024, out_features=256, bias=False)\n",
            "          (v_proj): Linear(in_features=1024, out_features=256, bias=False)\n",
            "          (o_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
            "        )\n",
            "        (mlp): LlamaMLP(\n",
            "          (gate_proj): Linear(in_features=1024, out_features=4096, bias=False)\n",
            "          (up_proj): Linear(in_features=1024, out_features=4096, bias=False)\n",
            "          (down_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
            "          (act_fn): SiLU()\n",
            "        )\n",
            "        (input_layernorm): LlamaRMSNorm((1024,), eps=1e-06)\n",
            "        (post_attention_layernorm): LlamaRMSNorm((1024,), eps=1e-06)\n",
            "      )\n",
            "    )\n",
            "    (norm): LlamaRMSNorm((1024,), eps=1e-06)\n",
            "    (rotary_emb): LlamaRotaryEmbedding()\n",
            "  )\n",
            "  (lm_head): Linear(in_features=1024, out_features=32000, bias=False)\n",
            ")\n",
            "Number of parameters: 248013824\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Number of parameters: 248013824 => 248M\n"
      ],
      "metadata": {
        "id": "CEGPrn0pIifH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create smaller model by remove hidden layers\n",
        "layers = model.model.layers\n",
        "model.model.layers = layers[:5] + layers[-5:]\n",
        "\n",
        "config = AutoConfig.from_pretrained(\n",
        "    model_name_or_path,\n",
        "    num_hidden_layers = len(model.model.layers)\n",
        ")\n",
        "\n",
        "model.config = config\n",
        "print_nparams(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v_Cuz6VwHdNw",
        "outputId": "5a37cb8c-63ce-4a74-e733-75b97d7ae8fc"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of parameters: 217601024\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Number of parameters: 217601024 => 217M\n"
      ],
      "metadata": {
        "id": "yTBidMhTInI3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Depth Upscaling from a general pretrained model\n"
      ],
      "metadata": {
        "id": "aOtukR72Iv8v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "config = LlamaConfig(\n",
        "    num_hidden_layers=16,\n",
        "    hidden_size=1024,\n",
        "    intermediate_size=4096,\n",
        "    num_attention_heads=32,\n",
        "    num_key_value_heads=8,\n",
        "    torch_dtype = 'bfloat16',\n",
        "    use_cache=False,\n",
        ")\n",
        "config"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nyh17_c4H-kd",
        "outputId": "b7886df6-aa86-4a33-fe46-078f100a0ffb"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LlamaConfig {\n",
              "  \"attention_bias\": false,\n",
              "  \"attention_dropout\": 0.0,\n",
              "  \"bos_token_id\": 1,\n",
              "  \"dtype\": \"bfloat16\",\n",
              "  \"eos_token_id\": 2,\n",
              "  \"head_dim\": 32,\n",
              "  \"hidden_act\": \"silu\",\n",
              "  \"hidden_size\": 1024,\n",
              "  \"initializer_range\": 0.02,\n",
              "  \"intermediate_size\": 4096,\n",
              "  \"max_position_embeddings\": 2048,\n",
              "  \"mlp_bias\": false,\n",
              "  \"model_type\": \"llama\",\n",
              "  \"num_attention_heads\": 32,\n",
              "  \"num_hidden_layers\": 16,\n",
              "  \"num_key_value_heads\": 8,\n",
              "  \"pretraining_tp\": 1,\n",
              "  \"rms_norm_eps\": 1e-06,\n",
              "  \"rope_scaling\": null,\n",
              "  \"rope_theta\": 10000.0,\n",
              "  \"tie_word_embeddings\": false,\n",
              "  \"transformers_version\": \"4.56.1\",\n",
              "  \"use_cache\": false,\n",
              "  \"vocab_size\": 32000\n",
              "}"
            ]
          },
          "metadata": {},
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = LlamaForCausalLM(config)\n",
        "model = model.to(dtype=torch.bfloat16)\n",
        "print_nparams(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AlZaS2d4JRim",
        "outputId": "c7bdb947-046a-485c-ed39-28b5af6cd310"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of parameters: 308839424\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Number of parameters: 308839424 => 308M"
      ],
      "metadata": {
        "id": "x_FNfsDdJkLq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_name_or_path = 'upstage/TinySolar-248m-4k'\n",
        "pretrained_model = AutoModelForCausalLM.from_pretrained(model_name_or_path, device_map='cpu', torch_dtype=torch.bfloat16)\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n",
        "print_nparams(pretrained_model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w1Z3JCsGJij6",
        "outputId": "71ce192a-840b-4567-eb05-9a9b8c7534c7"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of parameters: 248013824\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Number of parameters: 248013824 => 248M"
      ],
      "metadata": {
        "id": "cMA0dpPkKDbA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from copy import deepcopy\n",
        "\n",
        "model.model.layers = deepcopy(pretrained_model.model.layers[:-4]) + deepcopy(pretrained_model.model.layers[4:])\n",
        "model.model.embed_tokens = deepcopy(pretrained_model.model.embed_tokens)\n",
        "model.lm_head = deepcopy(pretrained_model.lm_head)\n",
        "\n",
        "print(model.config)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ATMqvl0dJcLt",
        "outputId": "d9f1cac3-b486-4dc7-ad9c-972480d667e1"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LlamaConfig {\n",
            "  \"attention_bias\": false,\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 1,\n",
            "  \"dtype\": \"bfloat16\",\n",
            "  \"eos_token_id\": 2,\n",
            "  \"head_dim\": 32,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"max_position_embeddings\": 2048,\n",
            "  \"mlp_bias\": false,\n",
            "  \"model_type\": \"llama\",\n",
            "  \"num_attention_heads\": 32,\n",
            "  \"num_hidden_layers\": 16,\n",
            "  \"num_key_value_heads\": 8,\n",
            "  \"pretraining_tp\": 1,\n",
            "  \"rms_norm_eps\": 1e-06,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 10000.0,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"transformers_version\": \"4.56.1\",\n",
            "  \"use_cache\": false,\n",
            "  \"vocab_size\": 32000\n",
            "}\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print_nparams(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4a9NK_s5KrGp",
        "outputId": "7070d9d8-eb6d-47a1-cd97-c7128c411f7a"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of parameters: 308839424\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Number of parameters: 308839424 => 308M (by deepcopy 2 248M models)"
      ],
      "metadata": {
        "id": "JesLKAdyKutr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = 'I am an engineer, i love'\n",
        "\n",
        "inputs = tokenizer(prompt, return_tensors='pt').to(model.device)\n",
        "streamer = TextStreamer(\n",
        "    tokenizer,\n",
        "    skip_prompt=True,\n",
        "    skip_special_tokens=False\n",
        "\n",
        ")\n",
        "\n",
        "outputs = model.generate(\n",
        "    **inputs,\n",
        "    streamer=streamer,\n",
        "    use_cache=True,\n",
        "    max_new_tokens=128,\n",
        "    do_sample=False\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q9BFJSWoKtCr",
        "outputId": "282a54be-f999-4d99-b5ca-7ddfcf47e47d"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "to use the word \"miscellaneous\" to describe the \"miscellaneous\" of a website.\n",
            "I have been a long-time user of the site, and I have been a fan of it. I have been a fan of the site, and I have been a fan of the site's history. I have been a fan of the site's history, and I have been a fan of the site's history.\n",
            "I have been a fan of the site's history, and I have been a fan of the site's history. I have been a fan of the site's\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model is initialized, but since it hasn't been pretrained, its responses aren't fluent. That's why pretraining is necessary after model preparation"
      ],
      "metadata": {
        "id": "SEBxB6odLOi3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# weight will be optimized at the time of pretraining, save the model for now\n",
        "model.save_pretrained('TinySolar-308m-4k-init')"
      ],
      "metadata": {
        "id": "le_YsblkLivr"
      },
      "execution_count": 85,
      "outputs": []
    }
  ]
}